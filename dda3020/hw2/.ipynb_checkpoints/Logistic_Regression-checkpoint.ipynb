{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Python and Scikit-Learn\n",
    "\n",
    "\n",
    "In this project, you will utilize Python and Scikit-Learn to implement Logistic Regression. Your task will involve constructing a classifier to predict tomorrow's rainfall in Australia by training a binary classification model using Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "The table of contents for this project is as follows:-\n",
    "\n",
    "\n",
    "1.\tIntroduction to Logistic Regression\n",
    "2.\tLogistic Regression intuition\n",
    "3.\tThe problem statement\n",
    "4.\tDataset description\n",
    "5.\tImport libraries\n",
    "6.\tImport dataset\n",
    "7.\tExploratory data analysis\n",
    "8.\tDeclare feature vector and target variable\n",
    "9.\tSplit data into separate training and test set\n",
    "10.\tFeature engineering\n",
    "11.\tFeature scaling\n",
    "12.\tModel training\n",
    "13.\tPredict results\n",
    "14.\tCheck accuracy score\n",
    "15.\tConfusion matrix\n",
    "16.\tClassification metrices\n",
    "17.\tAdjusting the threshold level\n",
    "18.\tROC - AUC\n",
    "19.\tRecursive feature elimination\n",
    "20.\tk-Fold Cross Validation\n",
    "21.\tHyperparameter optimization using GridSearch CV\n",
    "22.\tResults and conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Logistic Regression\n",
    "\n",
    "\n",
    "When data scientists may come across a new classification problem, the first algorithm that may come across their mind is **Logistic Regression**. It is a supervised learning classification algorithm which is used to predict observations to a discrete set of classes. Practically, it is used to classify observations into different categories. Hence, its output is discrete in nature. **Logistic Regression** is also called **Logit Regression**. It is one of the most simple, straightforward and versatile classification algorithms which is used to solve classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression intuition\n",
    "\n",
    "\n",
    "In statistics, the **Logistic Regression model** is a widely used statistical model which is primarily used for classification purposes. It means that given a set of observations, Logistic Regression algorithm helps us to classify these observations into two or more discrete classes. So, the target variable is discrete in nature.\n",
    "\n",
    "\n",
    "Logistic Regression algorithm works by implementing a linear equation with independent or explanatory variables to predict a response value. This predicted response value, denoted by z is then converted into a probability value that lie between 0 and 1. We use the **sigmoid function** in order to map predicted values to probability values. This sigmoid function then maps any real value into a probability value between 0 and 1. \n",
    "\n",
    "\n",
    "\n",
    "The sigmoid function returns a probability value between 0 and 1. This probability value is then mapped to a discrete class which is either “0” or “1”. In order to map this probability value to a discrete class (pass/fail, yes/no, true/false), we select a threshold value. This threshold value is called **Decision boundary**. Above this threshold value, we will map the probability values into class 1 and below which we will map values into class 0.\n",
    "\n",
    "\n",
    "Mathematically, it can be expressed as follows:-\n",
    "\n",
    "\n",
    "                    p ≥ 0.5 => class = 1\n",
    "    \n",
    "                    p < 0.5 => class = 0 \n",
    "\n",
    "\n",
    "Generally, the decision boundary is set to 0.5. So, if the probability value is 0.8 (> 0.5), we will map this observation to class 1.  Similarly, if the probability value is 0.2 (< 0.5), we will map this observation to class 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The problem statement\n",
    "\n",
    "\n",
    "In this project, you need to answer the question that whether or not it will rain tomorrow in Australia. \n",
    "\n",
    "\n",
    "To answer the question, you need to build a classifier to predict whether or not it will rain tomorrow in Australia by training a binary classification model using Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset description\n",
    "\n",
    "This dataset ``weatherAUS.csv`` contains daily weather observations from numerous Australian weather stations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'weatherAUS.csv'\n",
    "\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploratory data analysis\n",
    "\n",
    "\n",
    "Now, you will first explore the data to gain insights and understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142193, 24)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dimensions of dataset\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are 142193 instances and 24 variables in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the dataset\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df.columns\n",
    "\n",
    "col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop  RISK_MM variable\n",
    "\n",
    "You should first drop the `RISK_MM` feature variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['RISK_MM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view summary of dataset\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of variables\n",
    "\n",
    "\n",
    "In this section, you need to segregate the dataset into categorical and numerical variables. There are a mixture of categorical and numerical variables in the dataset. Categorical variables have data type object. Numerical variables have data type float64.\n",
    "\n",
    "\n",
    "First of all, you need to find categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :', categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the categorical variables\n",
    "\n",
    "df[categorical].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of categorical variables\n",
    "\n",
    "\n",
    "- There is a date variable. It is denoted by `Date` column.\n",
    "\n",
    "\n",
    "- There are 6 categorical variables. These are given by `Location`, `WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` and  `RainTomorrow`.\n",
    "\n",
    "\n",
    "- There are two binary categorical variables - `RainToday` and  `RainTomorrow`.\n",
    "\n",
    "\n",
    "- `RainTomorrow` is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore problems within categorical variables\n",
    "\n",
    "\n",
    "First, you will explore the categorical variables.\n",
    "\n",
    "\n",
    "### Missing values in categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables\n",
    "\n",
    "df[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables containing missing values\n",
    "\n",
    "cat1 = [var for var in categorical if df[var].isnull().sum()!=0]\n",
    "\n",
    "print(df[cat1].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are only 4 categorical variables in the dataset which contains missing values. These are `WindGustDir`, `WindDir9am`, `WindDir3pm` and `RainToday`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency counts of categorical variables\n",
    "\n",
    "\n",
    "Now, you can check the frequency counts of categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency of categorical variables\n",
    "\n",
    "for var in categorical: \n",
    "    \n",
    "    print(df[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency distribution of categorical variables\n",
    "\n",
    "for var in categorical: \n",
    "    \n",
    "    print(df[var].value_counts()/np.float(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of labels: cardinality\n",
    "\n",
    "\n",
    "The number of labels within a categorical variable is known as **cardinality**. A high number of labels within a variable is known as **high cardinality**. High cardinality may pose some serious problems in the machine learning model. So, you need to check for high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for cardinality in categorical variables\n",
    "\n",
    "for var in categorical:\n",
    "    \n",
    "    print(var, ' contains ', len(df[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is a `Date` variable which needs to be preprocessed. You should do preprocessing in the following section.\n",
    "\n",
    "\n",
    "All the other variables contain relatively smaller number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering of Date Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data type of `Date` variable is object. You should parse the date currently coded as object into datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the dates, currently coded as strings, into datetime format\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year from date\n",
    "\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "df['Year'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract month from date\n",
    "\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "df['Month'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract day from date\n",
    "\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "df['Day'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again view the summary of dataset\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are three additional columns created from `Date` variable. Now, I will drop the original `Date` variable from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original Date variable\n",
    "\n",
    "df.drop('Date', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the dataset again\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that the `Date` variable has been removed from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Categorical Variables\n",
    "\n",
    "\n",
    "Now, you will explore the categorical variables one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :', categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are 6 categorical variables in the dataset. The `Date` variable has been removed. First, you need to check missing values in categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in categorical variables \n",
    "\n",
    "df[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` variables contain missing values. You should explore these variables one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `Location` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of labels in Location variable\n",
    "\n",
    "print('Location contains', len(df.Location.unique()), 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in location variable\n",
    "\n",
    "df.Location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in Location variable\n",
    "\n",
    "df.Location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do One Hot Encoding of Location variable\n",
    "# get k-1 dummy variables after One Hot Encoding \n",
    "# preview the dataset with head() method\n",
    "\n",
    "pd.get_dummies(df.Location, drop_first=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of 1s per boolean variable over the rows of the dataset\n",
    "# it will tell us how many observations we have for each category\n",
    "\n",
    "pd.get_dummies(df.Location, drop_first=True, dummy_na=True).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is no missing value in WindGustDir variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `WindGustDir` variable\n",
    "\n",
    "**<font color=\"red\">[Task]</font>** Explore `WindGustDir` variable by displaying the number of unique labels, the count of each label, then perform One Hot Encoding, and finally determine the number of missing values in this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "# You can add more cells if necessary.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `WindDir9am` variable\n",
    "\n",
    "**<font color=\"red\">[Task]</font>** Explore `WindDir9am` variable by displaying the number of unique labels, the count of each label, then perform One Hot Encoding, and finally determine the number of missing values in this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "# You can add more cells if necessary.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `WindDir3pm` variable\n",
    "\n",
    "\n",
    "**<font color=\"red\">[Task]</font>** Explore `WindDir3pm` variable by displaying the number of unique labels, the count of each label, then perform One Hot Encoding, and finally determine the number of missing values in this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "# You can add more cells if necessary.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `RainToday` variable\n",
    "\n",
    "**<font color=\"red\">[Task]</font>** Explore `RainToday` variable by displaying the number of unique labels, the count of each label, then perform One Hot Encoding, and finally determine the number of missing values in this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "# You can add more cells if necessary.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find numerical variables\n",
    "\n",
    "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
    "\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "print('The numerical variables are :', numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the numerical variables\n",
    "\n",
    "df[numerical].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of numerical variables\n",
    "\n",
    "\n",
    "- There are 16 numerical variables. \n",
    "\n",
    "\n",
    "- These are given by `MinTemp`, `MaxTemp`, `Rainfall`, `Evaporation`, `Sunshine`, `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm`, `Humidity9am`, `Humidity3pm`, `Pressure9am`, `Pressure3pm`, `Cloud9am`, `Cloud3pm`, `Temp9am` and `Temp3pm`.\n",
    "\n",
    "\n",
    "- All of the numerical variables are of continuous type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore problems within numerical variables\n",
    "\n",
    "\n",
    "Now, you will explore the numerical variables.\n",
    "\n",
    "\n",
    "### Missing values in numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables\n",
    "\n",
    "df[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all the 16 numerical variables contain missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers in numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view summary statistics in numerical variables\n",
    "\n",
    "print(round(df[numerical].describe()),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On closer inspection, you can see that the `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns may contain outliers.\n",
    "\n",
    "\n",
    "You can draw boxplots to visualise outliers in the above variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw boxplots to visualize outliers\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = df.boxplot(column='Rainfall')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Rainfall')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = df.boxplot(column='Evaporation')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Evaporation')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "fig = df.boxplot(column='WindSpeed9am')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindSpeed9am')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "fig = df.boxplot(column='WindSpeed3pm')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindSpeed3pm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above boxplots confirm that there are lot of outliers in these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distribution of variables\n",
    "\n",
    "\n",
    "Now, you can plot the histograms to check distributions to find out if they are normal or skeyoud. If the variable follows normal distribution, then you will do `Extreme Value Analysis` otherwise if they are skeyoud, you will find IQR (Interquantile range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram to check distribution\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = df.Rainfall.hist(bins=10)\n",
    "fig.set_xlabel('Rainfall')\n",
    "fig.set_ylabel('RainTomorrow')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = df.Evaporation.hist(bins=10)\n",
    "fig.set_xlabel('Evaporation')\n",
    "fig.set_ylabel('RainTomorrow')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "fig = df.WindSpeed9am.hist(bins=10)\n",
    "fig.set_xlabel('WindSpeed9am')\n",
    "fig.set_ylabel('RainTomorrow')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "fig = df.WindSpeed3pm.hist(bins=10)\n",
    "fig.set_xlabel('WindSpeed3pm')\n",
    "fig.set_ylabel('RainTomorrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all the four variables are skeyoud. So, you will use interquantile range to find outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find outliers for Rainfall variable\n",
    "\n",
    "IQR = df.Rainfall.quantile(0.75) - df.Rainfall.quantile(0.25)\n",
    "Lower_fence = df.Rainfall.quantile(0.25) - (IQR * 3)\n",
    "Upper_fence = df.Rainfall.quantile(0.75) + (IQR * 3)\n",
    "print('Rainfall outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `Rainfall`, the minimum and maximum values are 0.0 and 371.0. So, the outliers are values > 3.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**<font color=\"red\">[Task]</font>** Use interquantile range to find outliers in the variables `Evaporation`, `WindSpeed9am`, `WindSpeed3pm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "# You can add more cells if necessary.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Declare feature vector and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['RainTomorrow'], axis=1)\n",
    "\n",
    "y = df['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Split data into separate training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of X_train and X_test\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering\n",
    "\n",
    "\n",
    "**Feature Engineering** is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. You will carry out feature engineering on different types of variables.\n",
    "\n",
    "\n",
    "First, you can display the categorical and numerical variables again separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types in X_train\n",
    "\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display categorical variables\n",
    "\n",
    "categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display numerical variables\n",
    "\n",
    "numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n",
    "\n",
    "numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering missing values in numerical variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables in X_train\n",
    "\n",
    "X_train[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables in X_test\n",
    "\n",
    "X_test[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentage of missing values in the numerical variables in training set\n",
    "\n",
    "for col in numerical:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print(col, round(X_train[col].isnull().mean(),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption\n",
    "\n",
    "\n",
    "We can assume that the data are missing completely at random (MCAR). There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation. When there are outliers in the dataset, you should use median imputation. So, you will use median imputation because median imputation is robust to outliers.\n",
    "\n",
    "\n",
    "You will impute missing values with the appropriate statistical measures of the data, in this case median. Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values in X_train with respective column median in X_train\n",
    "\n",
    "for col in numerical:\n",
    "    col_median=X_train[col].median()\n",
    "    X_train[col].fillna(col_median, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again missing values in numerical variables in X_train\n",
    "\n",
    "X_train[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">[Task]</font>** Impute missing values in X_test with respective column median in X_train and check again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that there are no missing values in the numerical columns of training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering missing values in categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentage of missing values in the categorical variables in training set\n",
    "\n",
    "X_train[categorical].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables with missing data\n",
    "\n",
    "for col in categorical:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print(col, (X_train[col].isnull().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing categorical variables with most frequent value\n",
    "\n",
    "X_train['WindGustDir'].fillna(X_train['WindGustDir'].mode()[0], inplace=True)\n",
    "X_train['WindDir9am'].fillna(X_train['WindDir9am'].mode()[0], inplace=True)\n",
    "X_train['WindDir3pm'].fillna(X_train['WindDir3pm'].mode()[0], inplace=True)\n",
    "X_train['RainToday'].fillna(X_train['RainToday'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_train\n",
    "\n",
    "X_train[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">[Task]</font>** Impute missing values in X_test with most frequent value in X_train and check again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check, you will check for missing values in X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_train\n",
    "\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_test\n",
    "\n",
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are no missing values in X_train and X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering outliers in numerical variables\n",
    "\n",
    "\n",
    "You have seen that the `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns contain outliers. You can use top-coding approach to cap maximum values and remove outliers from the above variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_value(df, variable, top):\n",
    "    return np.where(df[variable]>top, top, df[variable])\n",
    "\n",
    "X_train['Rainfall'] = max_value(X_train, 'Rainfall', 3.2)\n",
    "X_train['Evaporation'] = max_value(X_train, 'Evaporation', 21.8)\n",
    "X_train['WindSpeed9am'] = max_value(X_train, 'WindSpeed9am', 55)\n",
    "X_train['WindSpeed3pm'] = max_value(X_train, 'WindSpeed3pm', 57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">[Task]</font>** Use top-coding approach to cap maximum values and remove outliers  from the above variables in the X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Rainfall.max(), X_test.Rainfall.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Evaporation.max(), X_test.Evaporation.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.WindSpeed9am.max(), X_test.WindSpeed9am.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.WindSpeed3pm.max(), X_test.WindSpeed3pm.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[numerical].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see that the outliers in `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns are capped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode RainToday variable\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.BinaryEncoder(cols=['RainToday'])\n",
    "\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that two additional variables `RainToday_0` and `RainToday_1` are created from `RainToday` variable.\n",
    "\n",
    "Now, you will create the `X_train` training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train[numerical], X_train[['RainToday_0', 'RainToday_1']],\n",
    "                     pd.get_dummies(X_train.Location), \n",
    "                     pd.get_dummies(X_train.WindGustDir),\n",
    "                     pd.get_dummies(X_train.WindDir9am),\n",
    "                     pd.get_dummies(X_train.WindDir3pm)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you will create the `X_test` testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">[Task]</font>** Create the `X_test` testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have training and testing set ready for model building. Before that, you should map all the feature variables onto the same scale. It is called `feature scaling`. I will do it as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have `X_train` dataset ready to be fed into the Logistic Regression classifier. You will do it as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict_proba method\n",
    "\n",
    "\n",
    "**predict_proba** method gives the probabilities for the target variable(0 and 1) in this case, in array form.\n",
    "\n",
    "`0 is for probability of no rain` and `1 is for probability of rain.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of getting output as 0 - no rain\n",
    "\n",
    "logreg.predict_proba(X_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of getting output as 1 - rain\n",
    "\n",
    "logreg.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Check accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **y_test** are the true class labels and **y_pred_test** are the predicted class labels in the test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the train-set and test-set accuracy\n",
    "\n",
    "\n",
    "Now, you will compare the train-set and test-set accuracy to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = logreg.predict(X_train)\n",
    "\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(logreg.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training-set accuracy score is 0.8476 while the test-set accuracy to be 0.8501. These two values are quite comparable. So, there is no question of overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Logistic Regression, you use default value of C = 1. It provides good performance with approximately 85% accuracy on both the training and the test set. But the model performance on both the training and test set are very comparable. It is likely the case of underfitting. \n",
    "\n",
    "You will increase C and fit a more flexible model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">[Task]</font>** Implement the Logistic Regression with liblinear solver and C=100. Provide the accuracy for both X_train and X_test and determine whether overfitting or underfitting is occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what happens if you use more regularized model than the default value of C=1, by setting C=0.01.\n",
    "\n",
    "**<font color=\"red\">[Task]</font>** Implement the Logistic Regression with liblinear solver and C=0.01. Provide the accuracy for both X_train and X_test and determine whether overfitting or underfitting is occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model accuracy with null accuracy\n",
    "\n",
    "\n",
    "It is not enough to say that the model is very good based on the above accuracy. You must compare it with the **null accuracy**. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\n",
    "\n",
    "So, you should first check the class distribution in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution in test set\n",
    "\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the occurences of most frequent class is 22067. So, you can calculate null accuracy by dividing 22067 by total number of occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null accuracy score\n",
    "\n",
    "null_accuracy = (22067/(22067+6372))\n",
    "\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the model accuracy score is 0.8501 but null accuracy score is 0.7759. So, you can conclude that your Logistic Regression model is doing a very good job in predicting the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, based on the above analysis you can conclude that our classification model accuracy is very good. Your model is doing a very good job in terms of predicting the class labels.\n",
    "\n",
    "\n",
    "But, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making. \n",
    "\n",
    "\n",
    "You have another tool called `Confusion matrix` that comes to our rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Confusion matrix\n",
    "\n",
    "\n",
    "A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n",
    "\n",
    "\n",
    "Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n",
    "\n",
    "\n",
    "**True Positives (TP)** – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n",
    "\n",
    "\n",
    "**True Negatives (TN)** – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n",
    "\n",
    "\n",
    "**False Positives (FP)** – False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**\n",
    "\n",
    "\n",
    "\n",
    "**False Negatives (FN)** – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**\n",
    "\n",
    "\n",
    "\n",
    "These four outcomes are summarized in a confusion matrix given below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows `20892 + 3285 = 24177 correct predictions` and `3087 + 1175 = 4262 incorrect predictions`.\n",
    "\n",
    "\n",
    "In this case, we have\n",
    "\n",
    "\n",
    "- `True Positives` (Actual Positive:1 and Predict Positive:1) - 20892\n",
    "\n",
    "\n",
    "- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 3285\n",
    "\n",
    "\n",
    "- `False Positives` (Actual Negative:0 but Predict Positive:1) - 1175 `(Type I error)`\n",
    "\n",
    "\n",
    "- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 3087 `(Type II error)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Classification metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "\n",
    "**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model.\n",
    "\n",
    "You can print a classification report as follows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification accuracy\n",
    "\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification error\n",
    "\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "\n",
    "**Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). \n",
    "\n",
    "\n",
    "So, **Precision** identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n",
    "\n",
    "\n",
    "\n",
    "Mathematically, precision can be defined as the ratio of `TP to (TP + FP).`\n",
    "\n",
    "\n",
    "\n",
    "**<font color=\"red\">[Task]</font>** Please calculate the **precision**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print precision score\n",
    "\n",
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "\n",
    "Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes.\n",
    "It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). **Recall** is also called **Sensitivity**.\n",
    "\n",
    "\n",
    "**Recall** identifies the proportion of correctly predicted actual positives.\n",
    "\n",
    "\n",
    "Mathematically, recall can be given as the ratio of `TP to (TP + FN).`\n",
    "\n",
    "\n",
    "\n",
    "**<font color=\"red\">[Task]</font>** Please calculate the **recall**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate\n",
    "\n",
    "\n",
    "**True Positive Rate** is synonymous with **Recall**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_rate = TP / float(TP + FN)\n",
    "\n",
    "\n",
    "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1-score\n",
    "\n",
    "\n",
    "**f1-score** is the weighted harmonic mean of precision and recall. The best possible **f1-score** would be 1.0 and the worst \n",
    "would be 0.0.  **f1-score** is the harmonic mean of precision and recall. So, **f1-score** is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of `f1-score` should be used to \n",
    "compare classifier models, not global accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support\n",
    "\n",
    "\n",
    "**Support** is the actual number of occurrences of the class in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Adjusting the threshold level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities of two classes- 0 and 1\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)[0:10]\n",
    "\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "\n",
    "- In each row, the numbers sum to 1.\n",
    "\n",
    "\n",
    "- There are 2 columns which correspond to 2 classes - 0 and 1.\n",
    "\n",
    "    - Class 0 - predicted probability that there is no rain tomorrow.    \n",
    "    \n",
    "    - Class 1 - predicted probability that there is rain tomorrow.\n",
    "        \n",
    "    \n",
    "- Importance of predicted probabilities\n",
    "\n",
    "    - We can rank the observations by probability of rain or no rain.\n",
    "\n",
    "\n",
    "- predict_proba process\n",
    "\n",
    "    - Predicts the probabilities    \n",
    "    \n",
    "    - Choose the class with the highest probability    \n",
    "    \n",
    "    \n",
    "- Classification threshold level\n",
    "\n",
    "    - There is a classification threshold level of 0.5.    \n",
    "    \n",
    "    - Class 1 - probability of rain is predicted if probability > 0.5.    \n",
    "    \n",
    "    - Class 0 - probability of no rain is predicted if probability < 0.5.    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the probabilities in dataframe\n",
    "\n",
    "y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - No rain tomorrow (0)', 'Prob of - Rain tomorrow (1)'])\n",
    "\n",
    "y_pred_prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities for class 1 - Probability of rain\n",
    "\n",
    "logreg.predict_proba(X_test)[0:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 - Probability of rain\n",
    "\n",
    "y_pred1 = logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of predicted probabilities\n",
    "\n",
    "\n",
    "# adjust the font size \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "# plot histogram with 10 bins\n",
    "plt.hist(y_pred1, bins = 10)\n",
    "\n",
    "\n",
    "# set the title of predicted probabilities\n",
    "plt.title('Histogram of predicted probabilities of rain')\n",
    "\n",
    "\n",
    "# set the x-axis limit\n",
    "plt.xlim(0,1)\n",
    "\n",
    "\n",
    "# set the title\n",
    "plt.xlabel('Predicted probabilities of rain')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "\n",
    "- You can see that the above histogram is highly positive skewed.\n",
    "\n",
    "\n",
    "- The first column tell us that there are approximately 15000 observations with probability between 0.0 and 0.1.\n",
    "\n",
    "\n",
    "- There are small number of observations with probability > 0.5.\n",
    "\n",
    "\n",
    "- So, these small number of observations predict that there will be rain tomorrow.\n",
    "\n",
    "\n",
    "- Majority of observations predict that there will be no rain tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "y_pred1 = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "y_pred1 = y_pred1.reshape(-1,1)\n",
    "\n",
    "y_pred2 = binarize(y_pred1, threshold)\n",
    "\n",
    "y_pred2 = np.where(y_pred2 == 1, 'Yes', 'No')\n",
    "\n",
    "cm1 = confusion_matrix(y_test, y_pred2)\n",
    "    \n",
    "print ('With',threshold,'threshold the Confusion Matrix is ','\\n\\n',cm1,'\\n\\n',\n",
    "        \n",
    "        'with',cm1[0,0]+cm1[1,1],'correct predictions, ', '\\n\\n', \n",
    "        \n",
    "        cm1[0,1],'Type I errors( False Positives), ','\\n\\n',\n",
    "        \n",
    "        cm1[1,0],'Type II errors( False Negatives), ','\\n\\n',\n",
    "        \n",
    "        'Accuracy score: ', (accuracy_score(y_test, y_pred2)), '\\n\\n',\n",
    "        \n",
    "        'Sensitivity: ',cm1[1,1]/(float(cm1[1,1]+cm1[1,0])), '\\n\\n',\n",
    "        \n",
    "        'Specificity: ',cm1[0,0]/(float(cm1[0,0]+cm1[0,1])),'\\n\\n',\n",
    "        \n",
    "        '====================================================', '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">[Task]</font>** Evaluate your model with other threshold [0.2, 0.3, 0.4, 0.5]. Provide the Confusion Matrix, correct predictions, Type I errors, Type II errors, Accuracy score, Sensitivity and Specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "#\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "\n",
    "- In binary problems, the threshold of 0.5 is used by default to convert predicted probabilities into class predictions.\n",
    "\n",
    "\n",
    "- Threshold can be adjusted to increase sensitivity or specificity. \n",
    "\n",
    "\n",
    "- Sensitivity and specificity have an inverse relationship. Increasing one would always decrease the other and vice versa.\n",
    "\n",
    "\n",
    "- You should see that increasing the threshold level results in increased accuracy.\n",
    "\n",
    "\n",
    "- Adjusting the threshold level should be one of the last step you do in the model-building process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. ROC - AUC\n",
    "\n",
    "\n",
    "\n",
    "### ROC Curve\n",
    "\n",
    "\n",
    "Another tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \n",
    "classification threshold levels. \n",
    "\n",
    "\n",
    "\n",
    "The **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n",
    "\n",
    "\n",
    "\n",
    "**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN).`\n",
    "\n",
    "\n",
    "\n",
    "**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN).`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred1, pos_label = 'Yes')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.title('ROC curve for RainTomorrow classifier')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC  AUC\n",
    "\n",
    "\n",
    "**ROC AUC** stands for **Receiver Operating Characteristic - Area Under Curve**. It is a technique to compare classifier performance. In this technique, we measure the `area under the curve (AUC)`. A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. \n",
    "\n",
    "\n",
    "So, **ROC AUC** is the percentage of the ROC plot that is underneath the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ROC AUC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC = roc_auc_score(y_test, y_pred1)\n",
    "\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "\n",
    "- ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\n",
    "\n",
    "- ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it will rain tomorrow or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cross-validated ROC AUC \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Cross_validated_ROC_AUC = cross_val_score(logreg, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "print('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and improvement\n",
    "\n",
    "\n",
    "\n",
    "In this section, we will employ several techniques to improve the model performance. We will discuss 3 techniques which are used in practice for performance improvement. These are `recursive feature elimination`, `k-fold cross validation` and `hyperparameter optimization using GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Recursive Feature Elimination with Cross Validation\n",
    "\n",
    "\n",
    "`Recursive feature elimination (RFE)` is a feature selection technique that helps us to select best features from the given number of features. At first, the model is built on all the given features. Then, it removes the least useful predictor and build the model again. This process is repeated until all the unimportant features are removed from the model.\n",
    "\n",
    "\n",
    "`Recursive Feature Elimination with Cross-Validated (RFECV) feature selection` technique selects the best subset of features for the estimator by removing 0 to N features iteratively using recursive feature elimination. Then it selects the best subset based on the accuracy or cross-validation score or roc-auc of the model. Recursive feature elimination technique eliminates n features from a model by fitting the model multiple times and at each step, removing the weakest features.\n",
    "\n",
    "\n",
    "We will use this technique to select best features from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfecv = RFECV(estimator=logreg, step=1, cv=5, scoring='accuracy')\n",
    "\n",
    "rfecv = rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training data\n",
    "\n",
    "X_train_rfecv = rfecv.transform(X_train)\n",
    "\n",
    "\n",
    "# train classifier\n",
    "\n",
    "logreg.fit(X_train_rfecv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test classifier on test data\n",
    "\n",
    "X_test_rfecv = rfecv.transform(X_test)\n",
    "\n",
    "y_pred_rfecv = logreg.predict(X_test_rfecv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean accuracy on transformed test data and labels\n",
    "\n",
    "print (\"Classifier score: {:.4f}\".format(logreg.score(X_test_rfecv,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your original model accuracy score is 0.8501 whereas accuracy score after RFECV is 0.8500. So, you can obtain approximately similar accuracy but with reduced or optimal set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion-matrix revisited\n",
    "\n",
    "\n",
    "You can again plot the confusion-matrix for this model to get an idea of errors your model is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test, y_pred_rfecv)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm1)\n",
    "\n",
    "print('\\nTrue Positives(TP1) = ', cm1[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN1) = ', cm1[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP1) = ', cm1[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN1) = ', cm1[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in the original model, you have FP = 1175 whereas FP1 = 1174. So, you get approximately same number of false positives. Also, FN = 3087 whereas FN1 = 3091. So, you get slightly higher false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying 10-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(logreg, X_train, y_train, cv = 5, scoring='accuracy')\n",
    "\n",
    "print('Cross-validation scores:{}'.format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can summarize the cross-validation accuracy by calculating its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Average cross-validation score\n",
    "\n",
    "print('Average cross-validation score: {:.4f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your original model score is found to be 0.8476. The average cross-validation score is 0.8474. So, you can conclude that cross-validation does not result in performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Hyperparameter Optimization using GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = [{'penalty':['l1','l2']}, \n",
    "              {'C':[1, 10, 100, 1000]}]\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator = logreg,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the best model\n",
    "\n",
    "# best score achieved during the GridSearchCV\n",
    "print('GridSearch CV best score : {:.4f}\\n\\n'.format(grid_search.best_score_))\n",
    "\n",
    "# print parameters that give the best results\n",
    "print('Parameters that give the best results :','\\n\\n', (grid_search.best_params_))\n",
    "\n",
    "# print estimator that was chosen by the GridSearch\n",
    "print('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate GridSearch CV score on test set\n",
    "\n",
    "print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "\n",
    "- Your original model test accuracy is 0.8501 while GridSearch CV accuracy is 0.8507.\n",
    "\n",
    "\n",
    "- You can see that GridSearch CV improve the performance for this particular model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tThe logistic regression model accuracy score is 0.8501. So, the model does a very good job in predicting whether or not it will rain tomorrow in Australia.\n",
    "\n",
    "2.\tSmall number of observations predict that there will be rain tomorrow. Majority of observations predict that there will be no rain tomorrow.\n",
    "\n",
    "3.\tThe model shows no signs of overfitting.\n",
    "\n",
    "4.\tIncreasing the value of C results in higher test set accuracy and also a slightly increased training set accuracy. So, we can conclude that a more complex model should perform better.\n",
    "\n",
    "5.\tIncreasing the threshold level results in increased accuracy.\n",
    "\n",
    "6.\tROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it will rain tomorrow or not.\n",
    "\n",
    "7.\tOur original model accuracy score is 0.8501 whereas accuracy score after RFECV is 0.8500. So, we can obtain approximately similar accuracy but with reduced set of features.\n",
    "\n",
    "8.\tIn the original model, we have FP = 1175 whereas FP1 = 1174. So, we get approximately same number of false positives. Also, FN = 3087 whereas FN1 = 3091. So, we get slighly higher false negatives.\n",
    "\n",
    "9.\tOur, original model score is found to be 0.8476. The average cross-validation score is 0.8474. So, we can conclude that cross-validation does not result in performance improvement.\n",
    "\n",
    "10.\tOur original model test accuracy is 0.8501 while GridSearch CV accuracy is 0.8507. We can see that GridSearch CV improve the performance for this particular model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
